# 第八章：模型接入与配置

## 8.1 模型概述

### 8.1.1 Dify 与模型的关系

Dify 是基于大语言模型（LLM）的 AI 应用开发平台，模型是 Dify 的核心驱动力。Dify 本身不提供模型，而是作为一个统一的接口层，让你可以：

- 接入数百种不同的模型
- 统一管理模型配置
- 灵活切换和对比模型
- 控制模型调用成本

### 8.1.2 为什么需要模型管理

**多模型支持**：
不同的任务可能需要不同的模型，统一的管理界面让切换更便捷。

**成本控制**：
跟踪和优化模型调用成本，选择性价比最优的方案。

**灵活性**：
可以随时更换模型供应商，不被单一供应商锁定。

**安全性**：
集中管理 API 密钥，保护敏感信息。

### 8.1.3 模型类型分类

在 Dify 中，模型按使用场景分为四类：

| 类型 | 用途 | 示例模型 |
|-----|------|---------|
| 系统推理模型 | 应用核心推理 | GPT-4, Claude 3 |
| Embedding 模型 | 文本向量化 | text-embedding-3 |
| Rerank 模型 | 检索结果重排序 | Cohere Rerank |
| 语音模型 | 语音转文字 | Whisper |

## 8.2 模型供应商

### 8.2.1 主流模型供应商

**OpenAI**：
```yaml
公司: OpenAI
主要模型: GPT-4, GPT-3.5-turbo
特点:
  - 最广泛使用的商业模型
  - 优秀的指令跟随能力
  - 支持函数调用
获取 API Key: platform.openai.com
```

**Anthropic**：
```yaml
公司: Anthropic
主要模型: Claude 3 (Opus, Sonnet, Haiku)
特点:
  - 强大的长文本处理能力
  - 优秀的推理和分析能力
  - 注重安全和可控性
获取 API Key: console.anthropic.com
```

**Azure OpenAI**：
```yaml
公司: Microsoft Azure
主要模型: GPT-4, GPT-3.5-turbo (Azure 部署版)
特点:
  - 企业级服务保障
  - 合规性认证
  - 与 Azure 生态集成
获取方式: Azure Portal
```

### 8.2.2 国产模型供应商

**百度文心一言**：
```yaml
公司: 百度
主要模型: ERNIE-4.0, ERNIE-Bot
特点:
  - 中文理解能力强
  - 本土化服务
  - 多模态支持
获取 API Key: cloud.baidu.com/product/wenxinworkshop
```

**阿里通义千问**：
```yaml
公司: 阿里云
主要模型: qwen-turbo, qwen-plus, qwen-max
特点:
  - 优秀的中文能力
  - 长上下文支持
  - 与阿里云集成
获取 API Key: dashscope.aliyun.com
```

**讯飞星火**：
```yaml
公司: 科大讯飞
主要模型: Spark 3.5, Spark 3.0
特点:
  - 语音能力强
  - 多轮对话优化
  - 行业解决方案
获取 API Key: xinghuo.xfyun.cn
```

**智谱 AI**：
```yaml
公司: 智谱 AI
主要模型: GLM-4, ChatGLM
特点:
  - 开源与商用结合
  - 中英双语能力
  - 工具调用支持
获取 API Key: open.bigmodel.cn
```

### 8.2.3 开源模型部署

**Hugging Face**：
```yaml
平台: Hugging Face Inference API
主要模型: Llama, Mistral, Mixtral 等
特点:
  - 海量开源模型
  - 按需调用
  - 社区活跃
```

**本地部署方案**：

| 方案 | 特点 | 适用场景 |
|-----|------|---------|
| Ollama | 简单易用 | 本地开发测试 |
| vLLM | 高性能推理 | 生产环境 |
| LocalAI | OpenAI 兼容 | 本地替代 |
| Xinference | 多框架支持 | 灵活部署 |

## 8.3 模型接入配置

### 8.3.1 进入模型配置

1. 登录 Dify 控制台
2. 点击左下角"设置"
3. 选择"模型供应商"

### 8.3.2 接入 OpenAI

**配置步骤**：
1. 在模型供应商列表中找到 OpenAI
2. 点击"设置"按钮
3. 输入 API Key
4. 点击"保存"

**高级配置**：
```yaml
API Key: sk-xxxxxxxxxxxxxxxx
API Base URL: https://api.openai.com/v1  # 可选，用于代理
组织 ID: org-xxxxx  # 可选
```

**可用模型**：
- gpt-4-turbo
- gpt-4
- gpt-3.5-turbo
- text-embedding-3-small
- text-embedding-3-large
- whisper-1

### 8.3.3 接入 Anthropic

**配置步骤**：
1. 找到 Anthropic 供应商
2. 输入 API Key
3. 保存配置

**可用模型**：
- claude-3-opus
- claude-3-sonnet
- claude-3-haiku
- claude-2.1

### 8.3.4 接入 Azure OpenAI

**配置步骤**：
```yaml
API Base URL: https://your-resource.openai.azure.com
API Key: xxxxxxxxxx
API Version: 2024-02-15-preview
部署名称: your-deployment-name
```

**注意事项**：
- 需要先在 Azure Portal 创建资源
- 每个模型需要单独部署
- 部署名称需要手动指定

### 8.3.5 接入国产模型

**百度文心**：
```yaml
API Key: xxxxxxxxxxx
Secret Key: xxxxxxxxxxx
```

**阿里通义**：
```yaml
API Key: sk-xxxxxxxxxx
```

**讯飞星火**：
```yaml
App ID: xxxxxxxx
API Key: xxxxxxxxxx
API Secret: xxxxxxxxxxxx
```

### 8.3.6 接入本地模型

**Ollama 配置**：
```yaml
API Base URL: http://localhost:11434
模型名称: llama2, mistral, mixtral 等
```

**启动 Ollama**：
```bash
# 安装 Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 运行模型
ollama run llama2

# Ollama 服务地址
http://localhost:11434
```

**Xinference 配置**：
```yaml
API Base URL: http://localhost:9997
模型名称: 根据部署的模型确定
```

## 8.4 模型参数配置

### 8.4.1 核心参数说明

**Temperature（温度）**：
```yaml
范围: 0.0 - 2.0
默认: 1.0
作用: 控制输出的随机性
  - 低值(0.1-0.3): 输出更确定、更一致
  - 中值(0.5-0.7): 平衡创造性和一致性
  - 高值(0.8-1.5): 输出更多样、更有创意
推荐设置:
  - 代码生成: 0.1-0.3
  - 客服回答: 0.3-0.5
  - 创意写作: 0.7-1.0
```

**Top P（核采样）**：
```yaml
范围: 0.0 - 1.0
默认: 1.0
作用: 控制词汇选择的范围
  - 低值(0.5): 只选择概率最高的词
  - 高值(0.9): 允许选择更多词汇
注意: 通常不与 Temperature 同时调整
```

**Max Tokens（最大输出长度）**：
```yaml
作用: 限制模型输出的最大 Token 数
设置建议:
  - 简短回答: 256-512
  - 一般对话: 1024-2048
  - 长文生成: 4096+
注意: 更长的输出需要更多成本
```

**Presence Penalty（存在惩罚）**：
```yaml
范围: -2.0 - 2.0
默认: 0
作用: 惩罚已出现的词汇
  - 正值: 鼓励讨论新话题
  - 负值: 允许重复话题
```

**Frequency Penalty（频率惩罚）**：
```yaml
范围: -2.0 - 2.0
默认: 0
作用: 惩罚频繁出现的词汇
  - 正值: 减少重复用词
  - 负值: 允许重复用词
```

### 8.4.2 参数调优建议

**按场景调参**：

| 场景 | Temperature | Top P | Max Tokens |
|-----|-------------|-------|------------|
| 代码生成 | 0.2 | 0.9 | 2048 |
| 客服对话 | 0.5 | 0.9 | 512 |
| 创意写作 | 0.8 | 0.95 | 4096 |
| 数据分析 | 0.3 | 0.9 | 2048 |
| 翻译 | 0.3 | 0.9 | 2048 |
| 摘要 | 0.5 | 0.9 | 1024 |

### 8.4.3 模型切换

**在应用中切换模型**：
1. 进入应用编排界面
2. 点击模型选择下拉框
3. 选择目标模型
4. 调整相应参数
5. 测试效果

**模型对比测试**：
1. 准备测试用例
2. 在相同输入下对比不同模型
3. 记录响应质量、速度和成本
4. 选择最优方案

## 8.5 默认模型设置

### 8.5.1 设置默认模型

在"设置" → "模型供应商"中配置：

**系统推理模型**：
- 用于新建应用的默认推理模型
- 用于系统功能（对话命名、建议问题等）

**Embedding 模型**：
- 用于知识库向量化的默认模型

### 8.5.2 默认配置建议

```yaml
系统推理模型: gpt-3.5-turbo
  # 性价比高，适合大多数场景
  
Embedding 模型: text-embedding-3-small
  # 成本较低，效果良好
  
备选方案:
  推理: gpt-4-turbo（需要更强能力时）
  Embedding: text-embedding-3-large（需要更高质量时）
```

## 8.6 负载均衡

### 8.6.1 什么是负载均衡

当需要处理大量请求时，可以配置多个相同模型的 API Key，实现：

- 提高并发能力
- 避免单个 Key 的速率限制
- 故障转移

### 8.6.2 配置负载均衡

**步骤**：
1. 进入模型配置
2. 点击"负载均衡"
3. 添加多个 API Key
4. 配置负载策略

**负载策略**：
```yaml
轮询（Round Robin）: 按顺序依次使用
权重（Weighted）: 按权重比例分配
最少连接: 选择当前连接最少的
```

### 8.6.3 配置示例

```yaml
模型: gpt-4-turbo
负载均衡:
  - API Key: sk-key1
    权重: 50
    状态: 启用
  - API Key: sk-key2
    权重: 30
    状态: 启用
  - API Key: sk-key3
    权重: 20
    状态: 备用
```

## 8.7 成本控制

### 8.7.1 模型定价了解

**OpenAI 定价示例**（价格可能变化）：

| 模型 | 输入 | 输出 |
|-----|------|------|
| gpt-4-turbo | $10/1M tokens | $30/1M tokens |
| gpt-3.5-turbo | $0.5/1M tokens | $1.5/1M tokens |
| text-embedding-3-small | $0.02/1M tokens | - |

### 8.7.2 成本优化策略

**选择合适的模型**：
- 简单任务用 gpt-3.5-turbo
- 复杂任务才用 gpt-4

**优化提示词**：
- 精简提示词减少输入 Token
- 明确输出格式减少输出 Token

**合理设置参数**：
- 限制 max_tokens
- 避免不必要的长输出

**使用缓存**：
- 相似问题复用答案
- 减少重复调用

### 8.7.3 用量监控

在 Dify 中查看用量：
1. 进入应用详情
2. 查看"监控"面板
3. 分析 Token 消耗趋势

**监控指标**：
- 总 Token 消耗
- 平均每次调用 Token
- 成本趋势
- 高消耗请求

## 8.8 Embedding 模型配置

### 8.8.1 Embedding 模型的作用

Embedding 模型用于将文本转换为向量表示：

- 知识库文档向量化
- 用户查询向量化
- 相似度计算

### 8.8.2 主流 Embedding 模型

**OpenAI**：
```yaml
text-embedding-3-large:
  维度: 3072
  效果: 最佳
  成本: 较高
  
text-embedding-3-small:
  维度: 1536
  效果: 良好
  成本: 较低
```

**开源方案**：
```yaml
BGE（中文）:
  模型: bge-large-zh
  维度: 1024
  特点: 中文效果好
  
Jina Embeddings:
  模型: jina-embeddings-v2
  维度: 768
  特点: 多语言支持
```

### 8.8.3 选择建议

| 场景 | 推荐模型 | 原因 |
|-----|---------|------|
| 英文为主 | text-embedding-3-small | 性价比高 |
| 中文为主 | bge-large-zh | 中文优化 |
| 高精度要求 | text-embedding-3-large | 效果最好 |
| 预算有限 | 开源模型本地部署 | 成本最低 |

## 8.9 Rerank 模型配置

### 8.9.1 Rerank 的作用

Rerank 模型用于对检索结果进行重排序：

- 提高检索准确率
- 过滤不相关结果
- 优化答案质量

### 8.9.2 配置 Rerank

**Cohere Rerank**：
```yaml
提供商: Cohere
模型: rerank-multilingual-v2.0
配置:
  API Key: your-cohere-api-key
```

**Jina Reranker**：
```yaml
提供商: Jina AI
模型: jina-reranker-v1
配置:
  API Key: your-jina-api-key
```

### 8.9.3 使用建议

**适用场景**：
- 知识库检索质量要求高
- 检索结果需要精确排序
- 愿意承担额外成本

**配置位置**：
- 在知识库检索设置中启用
- 在工作流知识检索节点中配置

## 8.10 最佳实践

### 8.10.1 模型选择原则

**按任务选择**：
```yaml
简单对话: gpt-3.5-turbo
复杂推理: gpt-4
代码生成: gpt-4 或 Claude 3
长文处理: Claude 3
中文任务: 国产模型或专门优化的模型
```

**按成本选择**：
```yaml
开发测试: gpt-3.5-turbo
生产环境: 根据质量要求选择
高价值场景: 选择最优模型
```

### 8.10.2 安全建议

**API Key 管理**：
- 定期轮换 Key
- 不在代码中硬编码
- 使用环境变量

**访问控制**：
- 限制 Key 的访问 IP
- 设置使用配额
- 监控异常调用

### 8.10.3 可靠性保障

**故障转移**：
- 配置备用模型
- 使用负载均衡
- 监控模型可用性

**超时处理**：
- 设置合理的超时时间
- 配置重试策略
- 提供降级方案

## 8.11 本章小结

通过本章的学习，你应该掌握：

1. **模型分类**：了解不同类型模型的用途
2. **供应商接入**：掌握各主流供应商的接入方法
3. **参数配置**：理解核心参数的含义和调优方法
4. **成本控制**：学会监控和优化模型使用成本
5. **高可用**：了解负载均衡和故障转移配置

## 8.12 思考与练习

1. **实践练习**：
   - 接入 OpenAI 和一个国产模型
   - 对比不同模型在相同任务上的表现
   - 配置负载均衡

2. **思考题**：
   - 如何选择适合业务场景的模型？
   - 如何在质量和成本之间取得平衡？
   - 本地部署模型有哪些优缺点？

---

**下一章预告**：第九章将介绍 API 集成与发布，包括应用发布方式、API 调用和安全管理。
